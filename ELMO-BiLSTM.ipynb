{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjnQOBfaR0Wx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from future.utils import iteritems\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import add\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "from seqeval.metrics import f1_score, classification_report,accuracy_score\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json, model_from_yaml\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hior1LUYWbCu"
   },
   "outputs": [],
   "source": [
    "class SentenceGetterCoNLL(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                           s[\"pos\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class SentenceGetterGMB(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def read_gmb_data():\n",
    "    data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    words = list(set(data[\"Word\"].values))\n",
    "\n",
    "    words.append(\"EOL\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data[\"Tag\"].values))\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    getter = SentenceGetterGMB(data)\n",
    "    sentences = getter.sentences\n",
    "    # GMB MAX LEN\n",
    "    max_len = 81\n",
    "    tag2idx = {t: idx for idx, t in enumerate(tags)}\n",
    "\n",
    "    X = [[word[0] for word in s] for s in sentences]\n",
    "    x_tmp = []\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        for idx in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[idx])\n",
    "            except:\n",
    "                new_seq.append(\"PAD\")\n",
    "        x_tmp.append(new_seq)\n",
    "    X = n_tmp\n",
    "\n",
    "    y = [[tag2idx[word[2]] for word in s] for s in sentences]\n",
    "\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "    return np.array(X), np.array(y), tags, tag2idx, n_tags, max_len\n",
    "\n",
    "\n",
    "\n",
    "def read_conll_data(data_type = \"train\", tags_input = None, tag2idx_input = None):\n",
    "    if data_type == \"train\":\n",
    "        data = pd.read_csv(\"parsed_train.csv\")\n",
    "\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    words = list(set(data[\"word\"].values))\n",
    "\n",
    "    words.append(\"EOL\")\n",
    "    n_words = len(words)\n",
    "\n",
    "    \n",
    "    if data_type == \"train\":    \n",
    "        tags = list(set(data[\"tag\"].values))\n",
    "    else:\n",
    "        tags = tags_input\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    getter = SentenceGetterCoNLL(data)\n",
    "\n",
    "    sentences = getter.sentences\n",
    "    # CONLL MAX LEN\n",
    "    max_len = 140\n",
    "    if data_type == \"train\":\n",
    "        tag2idx = {t: idx for idx, t in enumerate(tags)}\n",
    "    else:\n",
    "        tags2idx = tag2idx_input\n",
    "\n",
    "\n",
    "    X = [[word[0] for word in sent] for sent in sentences]\n",
    "    x_tmp = []\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        for idx in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[idx])\n",
    "            except:\n",
    "                new_seq.append(\"PAD\")\n",
    "        x_tmp.append(new_seq)\n",
    "    X = x_tmp\n",
    "\n",
    "    y = [[tag2idx[word[2]] for word in sent] for sent in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "    return np.array(X), np.array(y), tags, tag2idx, n_tags, max_len\n",
    "\n",
    "def read_conll_data_test(tags, tag2idx):\n",
    "    \n",
    "    data = pd.read_csv(\"parsed_testa.csv\")\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    \n",
    "    words = list(set(data[\"word\"].values))\n",
    "\n",
    "    words.append(\"EOL\")\n",
    "    n_words = len(words)\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    getter = SentenceGetterCoNLL(data)\n",
    "\n",
    "    sentences = getter.sentences\n",
    "    # CONLL MAX LEN\n",
    "    max_len = 140\n",
    "\n",
    "    X = [[word[0] for word in sent] for sent in sentences]\n",
    "    x_tmp = []\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        for idx in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[idx])\n",
    "            except:\n",
    "                new_seq.append(\"PAD\")\n",
    "        new_X.append(new_seq)\n",
    "    X = new_X\n",
    "\n",
    "    y = [[tag2idx[word[2]] for word in sent] for sent in sentences]\n",
    "\n",
    "    \n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "\n",
    "def prepare_model(max_len, n_tags, lstm_units = 512, dropout=0.2, recurrent_dropout=0.2):\n",
    "    input_layer = Input(shape=(max_len,), dtype=tf.string)\n",
    "    \n",
    "    embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_layer)\n",
    "    \n",
    "    bi_dir = Bidirectional(LSTM(units=lstm_units, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "    \n",
    "    bi_dir_2 = Bidirectional(LSTM(units=lstm_units, return_sequences=True,\n",
    "                               recurrent_dropout=recurrent_dropout, dropout=dropout))(bi_dir)\n",
    "    x = add([x, bi_dir_2])  \n",
    "    output_layer = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute one of the two following cells to read CONLL or GMB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "TAG3292_HB91",
    "outputId": "a6f274e0-2057-4440-ecd5-946d9b4e2736"
   },
   "outputs": [],
   "source": [
    "# READ CONLL DATA\n",
    "X_tr, y_tr, tags, tag2idx, n_tags, max_len = read_conll_data(data_type = \"train\")\n",
    "X_te, y_te  = read_conll_data_test(tags, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7_x7zTNXBkp"
   },
   "outputs": [],
   "source": [
    "# READ GMB DATA\n",
    "# X_tr, y_tr, tags, tag2idx, n_tags, max_len = read_gmb_data()\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X_tr, y_tr, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ELMo embeddings from Tensorflow hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QuN-6TmcSwZD",
    "outputId": "7ef61ed1-20ad-45d1-b0c2-7cca28665bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "batch_size = 32\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute one of the two following cells to define the Train/Test dataset in order to fit the model and the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMB --> THIS WORKS OK\n",
    "\n",
    "# X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]\n",
    "# y_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]\n",
    "# y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "# y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "taB1bv-PSHLZ",
    "outputId": "658104d1-b8a6-4572-8854-910e14d168ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12800, 140), (12800, 140, 1), (2144, 140), (2144, 140, 1))"
      ]
     },
     "execution_count": 255,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONLL ---> THIS WORKS OK\n",
    "X_tr = X_tr[:14944]\n",
    "y_tr = y_tr[:14944]\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.14346895074, random_state=2018)\n",
    "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cgn-cXk7YAfG",
    "outputId": "ec0990d4-b9b2-476c-cab0-f5ba4f919e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model(max_len, n_tags, lstm_units = 512, dropout = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "4X32zLZnRLyw",
    "outputId": "9b08a148-199e-4072-8207-02ce211acab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 140)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, None, 1024)   0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, None, 1024)   6295552     lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, None, 1024)   6295552     bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, 1024)   0           bidirectional_20[0][0]           \n",
      "                                                                 bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 8)      8200        add_10[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 12,599,304\n",
      "Trainable params: 12,599,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Svg2a_wMboQG",
    "outputId": "cbee0ac6-1e01-4165-b0c0-8cf66fe994e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 2144 samples\n",
      "Epoch 1/5\n",
      "12800/12800 [==============================] - 316s 25ms/step - loss: 0.0222 - accuracy: 0.9936 - val_loss: 0.0058 - val_accuracy: 0.9982\n",
      "Epoch 2/5\n",
      "12800/12800 [==============================] - 313s 24ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0051 - val_accuracy: 0.9984\n",
      "Epoch 3/5\n",
      "12800/12800 [==============================] - 312s 24ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0037 - val_accuracy: 0.9988\n",
      "Epoch 4/5\n",
      "12800/12800 [==============================] - 311s 24ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0041 - val_accuracy: 0.9987\n",
      "Epoch 5/5\n",
      "12800/12800 [==============================] - 312s 24ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n",
    "                    batch_size=batch_size, epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute one of the following cells to test the model on GMB or CONLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FTNRoR_1mbAH",
    "outputId": "23240804-ff1d-4fdf-c5d1-6d85eef11995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456/3456 [==============================] - 52s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# TEST CONLL DATASET\n",
    "y_pred = model.predict(np.array(X_te[:3456]), verbose=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7fy4z_9mqhq"
   },
   "outputs": [],
   "source": [
    "# CONLL cont.\n",
    "pred_converted = y_pred\n",
    "pred_converted = np.argmax(pred_converted, axis=-1)\n",
    "y_te_converted = np.array(y_te[:3456], dtype = np.int64)\n",
    "\n",
    "tag_eval = list(tag2idx.values())\n",
    "tag_eval.remove(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fK7WBa_poCqQ"
   },
   "outputs": [],
   "source": [
    "# TEST GMB DATASET\n",
    "# y_pred = model.predict(np.array(X_te[:4768]), verbose=1) \n",
    "# pred_converted = y_pred\n",
    "# pred_converted = np.argmax(pred_converted, axis=-1)\n",
    "# y_te_converted = np.array(y_te[:4768], dtype = np.int64)\n",
    "\n",
    "\n",
    "# tag_eval = tag2idx.copy()\n",
    "# tag_eval.pop('O')\n",
    "# tag_eval = tag_eval.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results with F1-score. Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "kXiz0h9LoNjv",
    "outputId": "4d3d88a9-6412-4c63-df38-54cbb6f96a7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95      2093\n",
      "           1       0.98      0.98      0.98      3145\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.88      0.95      0.91      2078\n",
      "           6       0.90      0.90      0.90      1263\n",
      "           7       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.94      0.95      0.94      8583\n",
      "   macro avg       0.53      0.54      0.53      8583\n",
      "weighted avg       0.94      0.95      0.94      8583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EVAL\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "report = flat_classification_report(y_pred=pred_converted, y_true=y_te_converted, labels = tag_eval)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sRStkxS-92Hb",
    "outputId": "5be85750-09bb-4f29-8446-629ebce8138a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model w/ ELMO to disk!\n"
     ]
    }
   ],
   "source": [
    "model_yaml = model.to_yaml()\n",
    "with open(\"elmo_conll_1.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"elmo_conll_1.h5\")\n",
    "print(\"Saved model w/ ELMO to disk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "4Kyyn_NcALqi",
    "outputId": "20fd7767-a6e2-4326-c1cb-1b331fd08aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Word            Pred \n",
      "=========================\n",
      "As             :O    \n",
      "Harry          :I-PER\n",
      "rides          :O    \n",
      "the            :O    \n",
      "Hogwarts       :I-ORG\n",
      "Express        :I-ORG\n",
      "on             :O    \n",
      "his            :O    \n",
      "journey        :O    \n",
      "back           :O    \n",
      "from           :O    \n",
      "school         :O    \n",
      "after          :O    \n",
      "his            :O    \n",
      "fourth         :O    \n",
      "year           :O    \n",
      "and            :O    \n",
      "the            :O    \n",
      "dire           :O    \n",
      "Triwizard      :I-MISC\n",
      "Tournament,    :I-MISC\n",
      "he             :O    \n",
      "dreads         :O    \n",
      "disembarking   :O    \n",
      "from           :O    \n",
      "the            :O    \n",
      "train          :O    \n"
     ]
    }
   ],
   "source": [
    "# TRY TO LOAD THE MODEL \n",
    "\n",
    "yaml_file = open('elmo_conll_1.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml, custom_objects={\"elmo_model\": elmo_model, \"tf\": tf, \"hub\":hub, \"batch_size\": batch_size, \"max_len\": max_len})\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"elmo_conll_1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "sent = \"As Harry rides the Hogwarts Express on his journey back from school after his fourth year and the dire Triwizard Tournament, he dreads disembarking from the train\"\n",
    "\n",
    "sentences_to_predict = []\n",
    "words_to_predict = sent.split()\n",
    "input_pad = [\"PAD\"]\n",
    "len_max = 140\n",
    "words_to_predict.extend(input_pad * (len_max - len(words_to_predict)))\n",
    "\n",
    "sentences_to_predict.append(words_to_predict)\n",
    "\n",
    "# This is necessary because the model expects input with fixed length and batch size\n",
    "for idx in range(0, 31):\n",
    "    sentences_to_predict.append([\"PAD\"] * len_max)\n",
    "\n",
    "sentences_to_predict = np.array(sentences_to_predict)\n",
    "\n",
    "predicted_result = loaded_model.predict(sentences_to_predict)\n",
    "\n",
    "sentence_result = predicted_result[0]\n",
    "sentence_result = np.argmax(sentence_result, axis=-1)\n",
    "indices_to_tag_gmb = {0: 'B-geo', 1: 'B-tim', 2: 'I-gpe', 3: 'I-art', 4: 'B-per', 5: 'I-eve', 6: 'B-gpe', 7: 'I-geo', 8: 'B-eve', 9: 'I-nat', 10: 'B-nat', 11: 'I-org', 12: 'I-tim', 13: 'I-per', 14: 'B-org', 15: 'B-art', 16: 'O'}\n",
    "indices_to_tag_conll = {5: 'O', 1: 'I-PER', 4: 'I-ORG', 6: 'I-MISC', 0: 'I-LOC', 3: 'B-ORG', 7: 'B-MISC', 2: 'B-LOC'}\n",
    "\n",
    "\n",
    "print(\"{:15} {:5}\".format(\"Word\", \"Pred\"))\n",
    "print(\"=\"*25)\n",
    "for word, res in zip(words_to_predict, sentence_result):\n",
    "    if word != \"PAD\":\n",
    "        print(\"{:15}:{:5}\".format(word, indices_to_tag_conll[res]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "elmo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
